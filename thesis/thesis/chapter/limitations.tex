\chapter{Limitations}\label{chapter:limitations}
%

Along the way of developing and testing our model, we ran into some limitations that we want to discuss in this chapter.

\section{Reconstruction of Datasets}\label{DatasetLimitation}

As mentioned in Section~\ref{sec:datasets}, we originally wanted to use the Pathfinder and CVR datasets to evaluate our model on more complex data.
However, due to the way we implemented our auxiliary loss, using a decoder to reconstruct the input image, we ran into a lot of problems. 
These datasets consist of thin lines on a mostly blank (or white for the CVR dataset) background. Since most of the image is just blank space, the model quickly 
learns to just reconstruct a blank image to minimize the loss. The minimal loss of the thin lines is not enough encouragement for the model to actually learn to reconstruct the lines,
since the overall loss is already very low when reconstructing a blank image. 

We tried various loss functions, but they were either not enforcing a strong enough learning signal to avoid the local minima, or we couldn't use them 
for local losses around the glimpse, and could only run them on the full image, which also harms training since the model never saw some of the regions that it gets penalized on.

We are certain that these problems can be fixed. Be it either with a better way of training the decoder or with a different auxiliary task that is better suited for these datasets.
One of these different auxiliary losses is, for example, the Sensorimotor Reward from SUGARL \citep{shang2023activevisionreinforcementlearning}, where they punish gazes that do not contribute useful information,
which is measured in the accuracy of the decision-making policy.
However, that was not within the scope of this thesis, and therefore, we had to limit ourselves to the simpler maze dataset.

\section{Parameter and Training Efficiency}

On simpler tasks like the maze dataset, our model is pretty inefficient in terms of parameters and training time compared to the classifier model.
Due to the necessity of using an LSTM that keeps track of the seen glimpses, our model has a lot of parameters compared to other models that just use feedforward architectures.
Additionally, the training time is significantly higher since we have a lot of components that need to be trained together and the reinforcement learning through time steps 
is a lot noisier than a simple classification loss.

However, what we lack in efficiency we gain in flexibility, since our model could, for example, easily adapt to differently sized mazes without retraining or changing the architecture at all 
(except the decoder, which is not needed for classification).
Another major advantage is that our model is already set-up to work in a dynamic environment, since we already integrate information over time steps and can therefore directly be used in a setting
where the input is changing over time, like a video stream from a robot navigating in an environment or a game.