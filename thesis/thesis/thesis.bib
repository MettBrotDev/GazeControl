@misc{mnih2014recurrentmodelsvisualattention,
      title={Recurrent Models of Visual Attention}, 
      author={Volodymyr Mnih and Nicolas Heess and Alex Graves and Koray Kavukcuoglu},
      year={2014},
      eprint={1406.6247},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1406.6247}, 
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
  doi={10.1162/neco.1997.9.8.1735}
}
@misc{schulman2018highdimensionalcontinuouscontrolusing,
      title={High-Dimensional Continuous Control Using Generalized Advantage Estimation}, 
      author={John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
      year={2018},
      eprint={1506.02438},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1506.02438}, 
}
@misc{mnih2016asynchronousmethodsdeepreinforcement,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.01783}, 
}
@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}
@misc{pihlgren2024systematicperformanceanalysisdeep,
      title={A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions}, 
      author={Gustav Grund Pihlgren and Konstantina Nikolaidou and Prakash Chandra Chhipa and Nosheen Abid and Rajkumar Saini and Fredrik Sandin and Marcus Liwicki},
      year={2024},
      eprint={2302.04032},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.04032}, 
}
@ARTICLE{1284395,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}
  }
@misc{mathieu2016deepmultiscalevideoprediction,
      title={Deep multi-scale video prediction beyond mean square error}, 
      author={Michael Mathieu and Camille Couprie and Yann LeCun},
      year={2016},
      eprint={1511.05440},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.05440}, 
}
@misc{zerroug2022benchmarkcompositionalvisualreasoning,
      title={A Benchmark for Compositional Visual Reasoning}, 
      author={Aimen Zerroug and Mohit Vaishnav and Julien Colin and Sebastian Musslick and Thomas Serre},
      year={2022},
      eprint={2206.05379},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.05379}, 
}
@misc{tay2020longrangearenabenchmark,
      title={Long Range Arena: A Benchmark for Efficient Transformers}, 
      author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
      year={2020},
      eprint={2011.04006},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.04006}, 
}
@misc{pathfinder2019github,
  author       = {drewlinsley},
  title        = {Pathfinder},
  year         = {2019},
  howpublished = {\url{https://github.com/drewlinsley/pathfinder}},
  note         = {Accessed: 2025-11-07}
}
@misc{jaderberg2016reinforcementlearningunsupervisedauxiliary,
      title={Reinforcement Learning with Unsupervised Auxiliary Tasks}, 
      author={Max Jaderberg and Volodymyr Mnih and Wojciech Marian Czarnecki and Tom Schaul and Joel Z Leibo and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1611.05397},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.05397}, 
}
@misc{shang2023activevisionreinforcementlearning,
      title={Active Vision Reinforcement Learning under Limited Visual Observability}, 
      author={Jinghuan Shang and Michael S. Ryoo},
      year={2023},
      eprint={2306.00975},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.00975}, 
}
@inproceedings{krizhevsky2012imagenet,
  title        = {ImageNet Classification with Deep Convolutional Neural Networks},
  author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS) 25},
  year         = {2012},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}
}
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}
@article{BUSONIU20188,
title = {Reinforcement learning for control: Performance, stability, and deep approximators},
journal = {Annual Reviews in Control},
volume = {46},
pages = {8-28},
year = {2018},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2018.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1367578818301184},
author = {Lucian Buşoniu and Tim {de Bruin} and Domagoj Tolić and Jens Kober and Ivana Palunko},
keywords = {Reinforcement learning, Optimal control, Deep learning, Stability, Function approximation, Adaptive dynamic programming},
abstract = {Reinforcement learning (RL) offers powerful algorithms to search for optimal controllers of systems with nonlinear, possibly stochastic dynamics that are unknown or highly uncertain. This review mainly covers artificial-intelligence approaches to RL, from the viewpoint of the control engineer. We explain how approximate representations of the solution make RL feasible for problems with continuous states and control actions. Stability is a central concern in control, and we argue that while the control-theoretic RL subfield called adaptive dynamic programming is dedicated to it, stability of RL largely remains an open question. We also cover in detail the case where deep neural networks are used for approximation, leading to the field of deep RL, which has shown great success in recent years. With the control practitioner in mind, we outline opportunities and pitfalls of deep RL; and we close the survey with an outlook that – among other things – points out some avenues for bridging the gap between control and artificial-intelligence RL techniques.}
}
@article{ELMAN1990179,
title = {Finding structure in time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
year = {1990},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(90)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/036402139090002E},
author = {Jeffrey L. Elman},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.}
}
@article{hinton2006reducingdimensionalitydataneural,
author = {G. E. Hinton  and R. R. Salakhutdinov },
title = {Reducing the Dimensionality of Data with Neural Networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504-507},
year = {2006},
doi = {10.1126/science.1127647},
URL = {https://www.science.org/doi/abs/10.1126/science.1127647},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1127647},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}}
@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A. and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural Computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  doi={10.1162/089976600300015015}
}